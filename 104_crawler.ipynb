{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import os, os.path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL needed\n",
    "#### 104 Job Bank Website\n",
    "- Data Scientist\n",
    "- Data Analyst\n",
    "- Data Engineer\n",
    "- Machine Learning\n",
    "- Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base1 = 'https://www.104.com.tw/jobbank/joblist/joblist.cfm?jobsource=n104bank1&ro=0&keyword=data+scientist&order=1&asc=0&page='\n",
    "base2 = 'https://www.104.com.tw/jobbank/joblist/joblist.cfm?jobsource=n104bank1&ro=0&keyword=data+analyst&order=1&asc=0&page='\n",
    "base3 = 'https://www.104.com.tw/jobbank/joblist/joblist.cfm?jobsource=n104bank1&ro=0&keyword=data+engineer&order=1&asc=0&page='\n",
    "base4 = 'https://www.104.com.tw/jobbank/joblist/joblist.cfm?jobsource=n104bank1&ro=0&keyword=machine+learning&order=1&asc=0&page='\n",
    "base5 = 'https://www.104.com.tw/jobbank/joblist/joblist.cfm?jobsource=n104bank1&ro=0&keyword=data+mining&order=1&asc=0&page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict = {'Data Scientist': base1,\n",
    "       'Data Analyst': base2,\n",
    "       'Data Engineer': base3,\n",
    "       'Machine Learning': base4,\n",
    "       'Data Mining': base5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "for _,key in enumerate(dict):\n",
    "    with open(key+'.txt', 'w', encoding='utf-8') as outfile:\n",
    "        for i in np.arange(1,11,1):\n",
    "            url = dict[key] + str(i)\n",
    "            res = requests.get(url)\n",
    "            # \n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            # \n",
    "            job_names = soup.find_all(\"div\", class_='job_name')\n",
    "            a = []\n",
    "            for job_name in job_names:\n",
    "                a.append('https://www.104.com.tw/'+job_name.contents[0]['href'])\n",
    "            for ai in a:\n",
    "                res = requests.get(ai)\n",
    "                soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "                # \n",
    "                try:\n",
    "                    title = soup.find(\"meta\",  property=\"og:title\")\n",
    "                    print(title['content'][:-10])\n",
    "                except TypeError:\n",
    "                    print(\"\\n\")\n",
    "                else:\n",
    "                    counter = counter+1\n",
    "                    outfile.write(\"{}\\n\".format(title['content'][:-10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Process\n",
    "- Delete Duplicate\n",
    "- Split Title & Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Delete Duplicate \n",
    "with open('../CodingDog/crawler/All.txt', 'r', encoding='utf-8') as f:\n",
    "    seen = set()\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        count = count+1\n",
    "        line_lower = line.lower()\n",
    "        \n",
    "        if line_lower in seen:\n",
    "            print(line)\n",
    "        else:\n",
    "            seen.add(line_lower)\n",
    "\n",
    "## Split Title & Company           \n",
    "with open('../CodingDog/crawler/All_clean.csv', 'w', encoding='utf-8') as f:\n",
    "    for i in list(seen):\n",
    "        i = i.replace(',','_')\n",
    "        sp = i.rfind('_')\n",
    "        title= i[:sp]\n",
    "        company = i[(sp+1):]\n",
    "        f.write(title+\",\"+company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## count all \n",
    "print (count)\n",
    "\n",
    "## delete duplicate and find all\n",
    "print (len(seen)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
